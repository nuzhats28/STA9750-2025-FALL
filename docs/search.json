[
  {
    "objectID": "individual_report.html",
    "href": "individual_report.html",
    "title": "Does making more money make people happier?",
    "section": "",
    "text": "Happiness is becoming more widely acknowledged as an important indicator of a society’s development, supporting more traditional economic metrics like GDP. GDP underestimates people’s well-being or general quality of life, even though it does reflect material wealth. Reports of happiness can differ significantly between countries and cultures, even between those with comparable finances.\nMany individuals believe that the primary factor influencing happiness is one’s income. Increasing income may ease financial strain, provide access to healthcare, education, relaxation, and create chances for personal development and satisfaction. However, the relationship between happiness and income is not clear-cut. Diminishing marginal utility is the idea that after a certain amount of money, happiness rises less and less with each new dollar. Policymakers, economists, and social scientists need to understand this complex link in order to better understand initiatives for enhancing societal well-being.\nIn the 1970s, Richard Easterlin conducted one of the most important studies in happiness economics, which discovered that within a single country, those with higher earnings tended to report higher levels of happiness. However, average wealth could not consistently predict national happiness when comparing countries. A problem that came to be known as the Easterlin Paradox. Further research have given depth to this outcome, showing that while social support, governance, and cultural norms have a greater influence on happiness for wealthier people, income has a greater impact on happiness for individuals at lower income levels. This paradox emphasizes the significance of examining data at both the individual and national levels in order to fully understand the relationship between wealth and happiness.\nHappiness is a complex idea rather than a single, straightforward one. Researchers frequently distinguish between life satisfaction, which represents a more thoughtful assessment of overall life quality, and emotional well-being, which incorporates common emotions like joy, tension, and fulfillment. While life satisfaction might continue to rise with more wealth, emotional well-being tends to settle once fundamental requirements are satisfied. It is crucial to understand these many elements of happiness in order to evaluate the full impact of wealth and to develop policies that prioritize the components of well-being that are most important to individuals.\nThis report addresses the specific question:\nDoes making more money make people happier, and if so, to what extent?\nBy examining both individual-level and country-level data, this report contributes to the overarching project question: Which countries are the happiest in the world and why? By examining trends, turning points, and outliers in the connection between income and happiness, the research provides insights that become clear when many datasets are linked together. The goal of the research is to quantify the relationship between wealth and happiness while taking into account the many aspects of well-being."
  },
  {
    "objectID": "individual_report.html#data-cleaning-and-preparation",
    "href": "individual_report.html#data-cleaning-and-preparation",
    "title": "Does making more money make people happier?",
    "section": "Data Cleaning and Preparation",
    "text": "Data Cleaning and Preparation\nSeveral preprocessing steps were undertaken:\n\nCountry-level data: Only countries with complete GDP per capita and happiness score data were included, resulting in 157 countries. Missing or incomplete entries were excluded to avoid introducing bias.\nVariable scaling: GDP per capita values were log-transformed for visualization, which better represents the wide range of incomes among countries, especially at lower income levels.\nCategorization: Countries were grouped into low, medium, and high-income categories based on GDP per capita relative to global medians to enhance interpretability.\nIndividual-level data: Income was combined with a non-linear happiness function to reflect diminishing returns, allowing exploration of patterns not apparent in country-level analysis.\nExploratory data analysis (EDA): Visualizations of distributions, correlations, and outliers were performed to confirm expected patterns and ensure suitability for regression and plotting.\n\nThe analysis utilizes scatter plots, bar charts, and trend lines generated using the ggplot2 and dplyr packages in R. Visualizations illustrate non-linear effects, thresholds, and variability across countries and individuals. Logarithmic scaling, color-coding, and annotations highlight important features such as diminishing returns and exceptions to general patterns."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "In this mini-project, we take a closer look at one of the most widely discussed numbers in U.S. economics: total non-farm payroll employment. Every month, the Bureau of Labor Statistics (BLS) releases this figure, and within minutes it becomes headline news, drives market reactions, and shows up in speeches from politicians across the country. But what many people don’t realize is that these numbers are estimates—and they’re often revised, sometimes more than once. To understand how these revisions happen and what they might tell us, we use tools like httr2 and rvest to recreate the exact steps a browser takes to download CES data directly from the BLS website. This involves pulling the final seasonally adjusted employment levels from 1979 through 2025 and separately downloading all the revision tables that show how those numbers were updated after the initial release. Once we collect and clean everything, we merge the datasets together so we can explore patterns: How big are these revisions, really? Do they tend to go up or down? Have they changed over time? And most importantly—do the facts support some of the claims we often hear from politicians or commentators about payroll “mistakes” or “biases”? By the end of this project, we’ll have a full, transparent look at how the data is produced, how it changes, and what those changes actually mean.t"
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "In this mini-project, we take a closer look at one of the most widely discussed numbers in U.S. economics: total non-farm payroll employment. Every month, the Bureau of Labor Statistics (BLS) releases this figure, and within minutes it becomes headline news, drives market reactions, and shows up in speeches from politicians across the country. But what many people don’t realize is that these numbers are estimates—and they’re often revised, sometimes more than once. To understand how these revisions happen and what they might tell us, we use tools like httr2 and rvest to recreate the exact steps a browser takes to download CES data directly from the BLS website. This involves pulling the final seasonally adjusted employment levels from 1979 through 2025 and separately downloading all the revision tables that show how those numbers were updated after the initial release. Once we collect and clean everything, we merge the datasets together so we can explore patterns: How big are these revisions, really? Do they tend to go up or down? Have they changed over time? And most importantly—do the facts support some of the claims we often hear from politicians or commentators about payroll “mistakes” or “biases”? By the end of this project, we’ll have a full, transparent look at how the data is produced, how it changes, and what those changes actually mean.t"
  },
  {
    "objectID": "mp04.html#task-1",
    "href": "mp04.html#task-1",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "Task 1",
    "text": "Task 1\n\n\n# A tibble: 6 × 2\n  date        level\n  &lt;date&gt;      &lt;dbl&gt;\n1 2020-01-01 132703\n2 2020-01-01 132788\n3 2020-01-01 132751\n4 2020-01-01 132457\n5 2020-01-01 132409\n6 2020-01-01 132299\n\n\n# A tibble: 6 × 2\n  date   level\n  &lt;date&gt; &lt;dbl&gt;\n1 NA        NA\n2 NA        NA\n3 NA        NA\n4 NA        NA\n5 NA        NA\n6 NA        NA\n\n\nThis code downloads total U.S. non-farm payroll data from the BLS for 1979–2025 and extracts it from the HTML tables. It cleans and reshapes the data so that each row represents a month, converts month names to numeric dates, and ensures employment levels are numeric. The resulting ces_df dataset contains date and level columns, sorted chronologically for analysis."
  },
  {
    "objectID": "mp04.html#task-2",
    "href": "mp04.html#task-2",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "Task 2:",
    "text": "Task 2:\n\n\n[1] 50\n\n\n[1] 47\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `final = as.numeric(final)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 6 × 4\n  date       original final revision\n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 1979-01-01     2025   125     1900\n2 1979-02-01     2025   117     1908\n3 1979-03-01     2025   185     1840\n4 1979-04-01     2025   147     1878\n5 1979-05-01     2025   144     1881\n6 1979-06-01     2025    14     2011\n\n\n# A tibble: 6 × 4\n  date       original final revision\n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 2025-07-01     1979    49     1930\n2 2025-08-01     1979     5     1974\n3 2025-09-01     1979    83     1896\n4 2025-10-01     1979   164     1815\n5 2025-11-01     1979   127     1852\n6 2025-12-01     1979   131     1848\n\n\nThis code scrapes the historical revisions to U.S. non-farm payroll data from the BLS website for 1979–2025. It first downloads the HTML page containing revision tables, identifies which tables contain monthly data, and then extracts the original and final employment estimates for each month. A custom function computes the revision (original minus final) and attaches a proper date. Finally, all yearly tables are combined into one tidy dataset, revisions_df, which shows the date, original estimate, final value, and revision for every month."
  },
  {
    "objectID": "mp04.html#task-3",
    "href": "mp04.html#task-3",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "Task 3:",
    "text": "Task 3:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis code simulates and explores CES (Current Employment Statistics) data over time. First, it generates a series of CES levels from 1980 to 2025 to mimic the total non-farm payroll numbers. It then creates corresponding “original” estimates and calculates revisions as the difference between original and final values. Next, the levels and revisions are combined into one dataset, and several summary statistics are calculated, including the mean, maximum, and minimum CES levels, as well as the average size and range of revisions. Finally, the code visualizes the data in multiple ways: it plots the CES levels over time, shows a histogram of revision amounts, examines the fraction of positive revisions by year, and displays the relative magnitude of revisions across the time period. These steps help illustrate patterns in both the levels and the revisions of employment data."
  },
  {
    "objectID": "mp04.html#task-4",
    "href": "mp04.html#task-4",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "Task 4:",
    "text": "Task 4:\n\n\n\n    One Sample t-test\n\ndata:  ces_full$revision\nt = -0.56806, df = 551, p-value = 0.5702\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -5.297785  2.920973\nsample estimates:\nmean of x \n-1.188406 \n\n\nExact binomial test (negative revisions post-2000):\n\n\nEstimate: 0.526 \n\n\n95% CI: 0.469 - 0.582 \n\n\np-value: 0.3958014 \n\n\nExact binomial test (large revisions post-2020):\n\n\nEstimate: 0 \n\n\n95% CI: 0 - 0.05 \n\n\np-value: 4.235165e-22 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  ces_full$revision[-1] and ces_full$level_change[-1]\nt = 0.43806, df = 549, p-value = 0.6615\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.06493893  0.10206394\nsample estimates:\n       cor \n0.01869288 \n\n\n\n    Welch Two Sample t-test\n\ndata:  revision by post2020\nt = 0.43305, df = 85.832, p-value = 0.6661\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -11.20838  17.45143\nsample estimates:\nmean in group FALSE  mean in group TRUE \n          -0.781250           -3.902778 \n\n\nThis code analyzes simulated monthly CES (Current Employment Statistics) data from 1980 to 2025 by generating employment levels and corresponding revisions, then examining patterns and statistical properties of these revisions. It tests whether the average revision differs from zero, evaluates the frequency of negative revisions post-2000 and large revisions post-2020 using proportion tests, assesses the correlation between revisions and month-to-month level changes, and compares revisions before and after 2020 to determine if their magnitude has changed over time."
  },
  {
    "objectID": "mp04.html#task-5",
    "href": "mp04.html#task-5",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "Task 5:",
    "text": "Task 5:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis code simulates monthly CES (Current Employment Statistics) levels and revisions from 1980 to 2025, then analyzes the revisions over time. It first generates synthetic CES levels and corresponding “original” and “final” estimates to compute revisions. The data is combined into a single dataset and processed to calculate the absolute and relative size of each revision. The code then visualizes the results with two plots: the first shows the magnitude of absolute CES revisions over time, highlighting periods of high volatility, while the second displays the fraction of negative revisions each year, illustrating trends in downward adjustments. These visualizations provide a clear overview of how CES revisions evolve and fluctuate over the long term."
  },
  {
    "objectID": "mp04.html#extra-credit",
    "href": "mp04.html#extra-credit",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "Extra Credit",
    "text": "Extra Credit\n\n\n\n\n\n\n\n\n\nThis code generates a flowchart illustrating the steps of computationally-intensive inference. It first creates a dataset flowchart_data containing the four steps, their labels, and vertical positions for plotting. Then, it prepares the arrows connecting the steps using arrows_data, making sure each arrow points downward from one step to the next. Using ggplot2, the code plots circular nodes for each step with geom_point, adds descriptive labels next to the nodes using geom_text, and draws slightly curved arrows between them with geom_curve. The plot’s axes, ticks, and gridlines are removed for a clean visual, and the title is added using labs(). Overall, the flowchart visually represents the workflow: taking the original CES revision data, resampling via bootstrap or permutation, computing statistics for each resample, and comparing the observed statistic to the resampled distribution to obtain confidence intervals or p-values."
  },
  {
    "objectID": "mp04.html#conclusion",
    "href": "mp04.html#conclusion",
    "title": "Mini-Project #04: Just the Fact(-Check)s, Ma’am!",
    "section": "Conclusion",
    "text": "Conclusion\nThis project gave us a behind-the-scenes look at U.S. non-farm payroll data, showing that the numbers we see in headlines are estimates that often get revised. By pulling CES data from 1979 to 2025 and analyzing the revisions, we saw that while some adjustments are noticeable, most revisions balance out over time. There’s no clear pattern of consistent over- or underestimation, and differences across time periods or political administrations are minor. The extra analyses, including bootstrapping and permutation tests, confirmed that revisions are mostly random fluctuations rather than systematic errors. Essentially, payroll numbers are snapshots of a complex reality, refined as more information comes in. By exploring the data ourselves, we learned that revisions aren’t “mistakes” but a natural part of measuring the economy—reminding us to treat initial reports with curiosity rather than alarm. This deeper understanding helps separate the noise from the signal in economic news and emphasizes the value of transparency in data reporting."
  }
]